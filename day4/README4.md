### 12thJune Note
## referenceï¼šhttps://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_1d.py


# ğŸŒŸ Vision Transformer (ViT) ç»“æ„ä¸å®ç°ç¬”è®°

## ğŸ“ æ–‡ä»¶ä¸€ï¼šå›¾åƒç‰ˆ ViT å®ç°ï¼ˆä»¥ 2D å›¾åƒä¸ºè¾“å…¥ï¼‰

### ğŸ”§ æ¶æ„è¯´æ˜

#### PatchEmbedding
- åŠŸèƒ½ï¼šå°†å›¾åƒåˆ†å‰²ä¸º patch å¹¶å±•å¼€ä¸ºå‘é‡ï¼Œå†æŠ•å½±åˆ° `dim` ç»´ã€‚
- è¾“å…¥å°ºå¯¸ï¼š`[B, 3, H, W]`
- è¾“å‡ºå°ºå¯¸ï¼š`[B, N_patches, dim]`
- å®ç°æ–¹å¼ï¼š
  ```python
  Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size)
  ```

#### Attention
- åŠŸèƒ½ï¼šå®ç°å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚
- ç‰¹ç‚¹ï¼š
  - QKV å…¨éƒ¨ç”±ä¸€ä¸ªçº¿æ€§å±‚å¾—åˆ°ã€‚
  - ä½¿ç”¨ `einops` å¤„ç†ç»´åº¦ï¼Œå¢å¼ºå¯è¯»æ€§ã€‚

#### Transformer
- ç»„æˆï¼š
  - å¤šä¸ª encoder blockï¼ˆæ¯ä¸ªå« LayerNorm + Attention + FeedForwardï¼‰
  - æ®‹å·®è¿æ¥å’Œ LayerNorm é¡ºåºä¸æ ‡å‡† Transformer ä¸€è‡´ï¼ˆPost-LNï¼‰ã€‚

#### ViT ä¸»ç±»
- è¾“å…¥ï¼šå›¾åƒå¼ é‡ `[B, 3, 256, 256]`
- æ­¥éª¤ï¼š
  1. Patch åµŒå…¥ã€‚
  2. åŠ å…¥åˆ†ç±» token `cls_token`ã€‚
  3. åŠ ä¸Šä½ç½®ç¼–ç  `pos_embedding`ã€‚
  4. Transformer ç¼–ç ã€‚
  5. åˆ†ç±»å¤´è¾“å‡ºé¢„æµ‹ã€‚

### ğŸ§ª ç¤ºä¾‹ç”¨æ³•
```python
model = ViT(image_size=256, patch_size=16, num_classes=100, ...)
img = torch.randn(1, 3, 256, 256)
logits = model(img)  # è¾“å‡º [1, 100]
```

---

## ğŸ“ æ–‡ä»¶äºŒï¼šä¸€ç»´æ—¶é—´åºåˆ—ç‰ˆ ViT å®ç°ï¼ˆç”¨äº 1D ä¿¡å·ï¼‰

### ğŸ“Œ è¾“å…¥ç»“æ„
- è¾“å…¥ shapeï¼š`[B, C, L]`ï¼Œä¾‹å¦‚ `[4, 3, 256]`
- å°†ä¸€ç»´ä¿¡å·åˆ’åˆ†ä¸º `patch_size` é•¿åº¦çš„ç‰‡æ®µã€‚
- æ¯ä¸ª patch æ˜¯ `[patch_size Ã— C]`ï¼Œåæ¥ Linear æ˜ å°„åˆ° `dim`ã€‚

### ğŸ”§ PatchEmbeddingï¼ˆæ—¶é—´åºåˆ—ç‰ˆï¼‰
```python
Rearrange('b c (n p) -> b n (p c)', p=patch_size)
```

### ğŸ”„ åˆ†ç±» Token æ‹¼æ¥æ–¹å¼
- ç”±äºè¾“å…¥æ˜¯åºåˆ—ï¼Œåˆ†ç±» token æ˜¯ `[dim]`ï¼Œç›´æ¥é€šè¿‡ `repeat` æ‰©å±•ä¸º `[B, dim]`ã€‚
- ä½¿ç”¨ `einops.pack` å’Œ `unpack` æ¥ç»„åˆå’Œæå–åˆ†ç±» tokenã€‚

### ğŸ§  Transformer ä¸ FeedForward
ç»“æ„ä¸å›¾åƒç‰ˆæœ¬ä¸€è‡´ã€‚

### âœ… è¾“å‡º
è¿”å› `[B, num_classes]` çš„ logits é¢„æµ‹ã€‚

---
